### 梯度消失问题 ###
反向传播函数上，当轮数网络层数增多，出现，可以使用ReLU激活函数
一定程度上代替sigmoid函数，
#### GoogLeNet ####
三个输出（不同层的输出，为了解决梯度消失问题
减小通道数，增加一个1*1的卷积核。在输入后开始进行1*1卷积为了降低特征值的稀疏性（个人理解），降维，输出为通道为卷积核个数，
3*3卷积填充1,5*5填充2，大小保持不变，每一层中的卷积核大小不一样，1*1,3*3,5*5可以抽取不同粒度的特征，通道数占比，1/4,1/2,1/8,还有1/8是输入层最大池化后，经过1*1卷积核通道数占比。
第4个inception模块层有输出，最后输出层，最后输出用7*7的平均池化代替，无填充，再送入全连接层
中间两个输出为5*5的平均池化。
#### ResNet ####

### 围棋中的深度强化学习 ###
强化学习：学习者不知道如何做，通过尝试发现哪些动作会产生最大收益。两个特征：试错和延迟收益。

深度强化学习：神经网络训练，将收益转换成标注，不能获取即正确又有代表性的示列。
关键是损失函数的定义，自我博弈，最小化损失函数来得到结果。

#### 策略网络 ####
基于策略梯度的强化学习，自我博弈产生
	（s,a,Pa,Ta),S代表当前棋局，a代表下一步落子的地方，Pa代表获胜的概率，Ta为胜负值（1，-1），使用交叉熵损失函数最小

![](E:/仓库管理/gitee/machine-learning/png/celvetidu.png)

基于价值评估的强化学习

